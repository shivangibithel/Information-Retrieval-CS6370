{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhFBL2QUOaaL"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw7f4PzHe1hM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/IR\\ group2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n2J4gmlei88"
      },
      "source": [
        "import pickle\n",
        "with open('datalist.txt', \"rb\") as fp:\n",
        "    dataset=pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr2I2xBfL8qM"
      },
      "source": [
        "doc=[]\r\n",
        "for i in dataset:\r\n",
        "    doc.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKsYpzfLewMP"
      },
      "source": [
        "import pickle\n",
        "with open('querylist.txt', \"rb\") as fp:\n",
        "    qrs=pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VH1hbtIMVxF"
      },
      "source": [
        "query_vec=[]\r\n",
        "for i in qrs:\r\n",
        "    query_vec.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzPM_sf7MiM0"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnXWlOqLMhKs"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from collections import Counter\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "import nltk\r\n",
        "import os\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import copy\r\n",
        "import pandas as pd\r\n",
        "import pickle\r\n",
        "import re\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxsgxRlVMxEM"
      },
      "source": [
        "def convert_lower_case(data):\r\n",
        "    return np.char.lower(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTXFNfSdM9f0"
      },
      "source": [
        "def remove_stop_words(data):\r\n",
        "    stop_words = stopwords.words('english')\r\n",
        "    words = word_tokenize(str(data))\r\n",
        "    new_text = \"\"\r\n",
        "    for w in words:\r\n",
        "        if w not in stop_words and len(w) > 1:\r\n",
        "            new_text = new_text + \" \" + w\r\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC4ig_nnNB2D"
      },
      "source": [
        "def remove_punctuation(data):\r\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\r\n",
        "    for i in range(len(symbols)):\r\n",
        "        data = np.char.replace(data, symbols[i], ' ')\r\n",
        "        data = np.char.replace(data, \"  \", \" \")\r\n",
        "    data = np.char.replace(data, ',', '')\r\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWjz_nDuNHo7"
      },
      "source": [
        "def remove_apostrophe(data):\r\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOkAZB4tNJDM"
      },
      "source": [
        "def stemming(data):\r\n",
        "    stemmer= PorterStemmer()\r\n",
        "    \r\n",
        "    tokens = word_tokenize(str(data))\r\n",
        "    new_text = \"\"\r\n",
        "    for w in tokens:\r\n",
        "        new_text = new_text + \" \" + stemmer.stem(w)\r\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghoxcH_ZPV-s"
      },
      "source": [
        "def remove_numbers(data):\r\n",
        "  filtered_word_list = []\r\n",
        "  for word in data:\r\n",
        "    if (( len(word) >= 3 and word.isalpha() and word.lower() not in stop_words )):\r\n",
        "      filtered_word_list.append(word)\r\n",
        "\r\n",
        "  return filtered_word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE6WLvm9NM5D"
      },
      "source": [
        "def preprocess(data):\r\n",
        "    data = convert_lower_case(data)\r\n",
        "    data = remove_punctuation(data) #remove comma seperately\r\n",
        "    data = remove_apostrophe(data)\r\n",
        "    data = remove_stop_words(data)\r\n",
        "    data = stemming(data)\r\n",
        "    data = remove_punctuation(data)\r\n",
        "    data = stemming(data) \r\n",
        "    data = remove_punctuation(data) \r\n",
        "    data = remove_stop_words(data) \r\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3PjjGV7PXkU"
      },
      "source": [
        "processed_document = []\r\n",
        "for i in range(len(doc)):\r\n",
        "    processed_document.append(preprocess(doc[i]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghy8GG1a82Oz"
      },
      "source": [
        "filtered_document=[]\r\n",
        "for i in range(len(processed_document)):\r\n",
        "    case_words =processed_document[i].split(\" \")\r\n",
        "    filtered_word_list = [word for word in case_words if (( len(word) >= 3 and word.isalpha() ))  ]\r\n",
        "    filtered_word_list=\" \".join(filtered_word_list)\r\n",
        "    filtered_document.append(filtered_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}